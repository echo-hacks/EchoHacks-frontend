<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8"/>
    <link rel="shortcut icon" href="%PUBLIC_URL%/favicon.png"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <meta name="theme-color" content="#000000"/>
    <!--
      manifest.json provides metadata used when your web app is installed on a
      user's mobile device or desktop. See https://developers.google.com/web/fundamentals/web-app-manifest/
    -->
    <link rel="manifest" href="%PUBLIC_URL%/manifest.json"/>
    <!--
      Notice the use of %PUBLIC_URL% in the tags above.
      It will be replaced with the URL of the `public` folder during the build.
      Only files inside the `public` folder can be referenced from the HTML.

      Unlike "/favicon.ico" or "favicon.ico", "%PUBLIC_URL%/favicon.ico" will
      work correctly both with client-side routing and a non-root public URL.
      Learn how to configure a non-root public URL by running `npm run build`.
    -->
    <title>EchoHacks</title>
    
</head>
<body>
<!-- <noscript>You need to enable JavaScript to run this app.</noscript> -->

<canvas id="voice" width="1000" height="160"></canvas>

<div id="root"></div>
<!--
  This HTML file is a template.
  If you open it directly in the browser, you will see an empty page.

  You can add webfonts, meta tags, or analytics to this file.
  The build step will place the bundled scripts into the <body> tag.

  To begin the development, run `npm start` or `yarn start`.
  To create a production bundle, use `npm run build` or `yarn build`.
-->
<script>

// pinch = 음정 값. 0~512. 소리가 너무 작을경우 -1
// volume = 소리 크기 값. 0~255 
// onDataReceived : 125ms마다 실시간으로 인식된 값을 전달함
var pinchList = []
var volumeList = []
function onDataReceived(pinch, volume) {
    console.log(pinch, volume)
    pinchList.push(pinch)
    volumeList.push(volume)

    var canvas = document.getElementById('voice');
    var ctx = canvas.getContext('2d');
    ctx.clearRect(0, 0, canvas.width, canvas.height);

      ctx.fillStyle = "rgba(110, 180, 227)";

      for (var i=0;i<volumeList.length;i++){
        ctx.fillRect(i*5,100 - volumeList[i] / 2,5,volumeList[i])
      }
     ctx.fillStyle = "rgb(26,15,53)";
      ctx.beginPath();
      ctx.moveTo(0, 150-pinchList[0]/2);
      for (var i=1;i<pinchList.length-1;i++){
      ctx.lineTo(i*5, 150-pinchList[i]/2);
      }
      // ctx.moveTo(pinchList.length*10, pinchList[pinchList.length-1]);
    ctx.stroke();
      if(pinchList.length > 100) {
        pinchList.shift();
        volumeList.shift();
      }
      
}

// onGraphDataReceived : 3초마다 125ms 단위로 기록된 24개의 데이터를 리턴함
function onGraphDataReceived(pinchList, volumeList) {
  console.log(pinchList, volumeList)
}

//////////////////////////////////////////////////////
// 음성데이터 추출 부분
//////////////////////////////////////////////////////
function Microphone (_fft) {

  function map(value, flag, peak_volume, min, max) {
    return [value, flag, peak_volume, min, max]
  }

  var FFT_SIZE = _fft || 1024;

  this.spectrum = new Uint8Array(FFT_SIZE/2);
  this.data = [];
  this.volume = this.vol = 0;
  this.peak_volume = 0;

  var self = this;
  var audioContext = new AudioContext();

  var SAMPLE_RATE = audioContext.sampleRate;
  window.AudioContext = window.AudioContext || window.webkitAudioContext;
  navigator.getUserMedia = navigator.getUserMedia || navigator.webkitGetUserMedia;

  window.addEventListener('load', init, false);

  function init () {
    try {
      startMic(new AudioContext());
    }
    catch (e) {
      console.error(e);
      alert('Web Audio API is not supported in this browser');
    }
  }

  var pinchList = []
  var volumeList = []
  function startMic (context) {

    navigator.getUserMedia({ audio: true }, processSound, error);

    function processSound (stream) {

      console.log("processSound")
      // analyser extracts frequency, waveform, and other data
      var analyser = context.createAnalyser();
      analyser.smoothingTimeConstant = 0.2;
      analyser.fftSize = FFT_SIZE;

      var node = context.createScriptProcessor(FFT_SIZE*2, 1, 1);
      node.onaudioprocess = function () {
        // console.log("bass" + self.getBass())
        // getByteFrequencyData returns the amplitude for each frequency
        // mixData = self.getMix()
        // bass = self.getMapRMS(mixData.bass)
        // mid = self.getMapRMS(mixData.mids)
        // high = self.getMapRMS(mixData.highs)
        // console.log("bass : " + parseInt(bass) + " mid : " + parseInt(mid) + " high : " + parseInt(high))
        // console.log("spectrum : " + self.spectrum)


        analyser.getByteFrequencyData(self.spectrum);
        self.data = adjustFreqData(self.spectrum);
        
        // getByteTimeDomainData gets volumes over the sample time
        //analyser.getByteTimeDomainData(dataArray);
        self.vol = self.getRMS(self.spectrum);
        // get peak
        if (self.vol > self.peak_volume) self.peak_volume = self.vol;
        self.volume = self.vol;


        max = -1
        index = -1
        threshold = 90
        for (var i = 0; i < self.spectrum.length; i++) {
          if (self.spectrum[i] < threshold) continue;
          if(max < self.spectrum[i]) {
            max = self.spectrum[i]
            index = i
          }
        }
        pinch = index

        pinchList.push(pinch);
        volumeList.push(self.volume);
        onDataReceived(pinch, self.volume);
        if (pinchList.length >=8 * 3) {
          onGraphDataReceived(pinchList, volumeList);
          pinchList = [];
          volumeList = [];
        }
      };

      var input = context.createMediaStreamSource(stream);

      input.connect(analyser);
      analyser.connect(node);
      node.connect(context.destination);

    }

    function error () {
      console.log("error")
      console.log(arguments);
    }

  }

  ///////////////////////////////////////////////
  ////////////// SOUND UTILITIES  //////////////
  /////////////////////////////////////////////
  this.mapSound = function(_me, _total, _min, _max){

    if (self.spectrum.length > 0) {

      var min = _min || 0;
      var max = _max || 100;
      //actual new freq
      var new_freq = Math.floor(_me /_total * self.data.length);
      //console.log(self.spectrum[new_freq]);
      // map the volumes to a useful number
      return map(self.data[new_freq], 0, self.peak_volume, min, max);
    } else {
      return 0;
    }

  }

  this.mapRawSound = function(_me, _total, _min, _max){

    if (self.spectrum.length > 0) {

      var min = _min || 0;
      var max = _max || 100;
      //actual new freq
      var new_freq = Math.floor(_me /_total * (self.spectrum.length)/2);
      //console.log(self.spectrum[new_freq]);
      // map the volumes to a useful number
      return map(self.spectrum[new_freq], 0, self.peak_volume, min, max);
    } else {
      return 0;
    }

  }

  this.getVol = function(){

    // map total volume to 100 for convenience
    self.volume = map(self.vol, 0, self.peak_volume, 0, 100);
    return self.volume;
  }

  this.getVolume = function() { return this.getVol();}

  //A more accurate way to get overall volume
  this.getRMS = function (spectrum) {
        var rms = 0;
        for (var i = 0; i < spectrum.length; i++) {
          rms += spectrum[i] * spectrum[i];
        }
        rms /= spectrum.length;
        rms = Math.sqrt(rms);
        return rms;
  }
  this.getMapRMS = function (spectrum) {
        var rms = 0;
        for (var i = 0; i < spectrum.length; i++) {
          rms += spectrum[i][0] * spectrum[i][0];
        }
        rms /= spectrum.length;
        rms = Math.sqrt(rms);
        return rms;
  }

  //freq = n * SAMPLE_RATE / MY_FFT_SIZE
  function mapFreq(i){
    var freq = i * SAMPLE_RATE / FFT_SIZE;
    return freq;
  }

  // getMix function. Computes the current frequency with
  // computeFreqFromFFT, then returns bass, mids and his
  // sub bass : 0 > 100hz
  // mid bass : 80 > 500hz
  // mid range: 400 > 2000hz
  // upper mid: 1000 > 6000hz
  // high freq: 4000 > 12000hz
  // Very high freq: 10000 > 20000hz and above

  this.getMix = function(){
    var highs = [];
    var mids = [];
    var bass = [];
    for (var i = 0; i < self.spectrum.length; i++) {
      var band = mapFreq(i);
      var v = map(self.spectrum[i], 0, self.peak_volume, 0, 100);
      if (band < 500) {
        bass.push(v);
      }
      if (band > 400 && band < 6000) {
          mids.push(v);
      }
      if (band > 4000) {
          highs.push(v);
      }
    }
    //console.log(bass);
    return {bass: bass, mids: mids, highs: highs}
  }


  this.getBass = function(){
        return this.getMix().bass;
  }

  this.getMids = function(){
      return this.getMix().mids;
  }

  this.getHighs = function(){
      return this.getMix().highs;
  }

  this.getHighsVol = function(_min, _max){
  var min = _min || 0;
  var max = _max || 100;
  var v = map(this.getRMS(this.getMix().highs), 0, self.peak_volume, min, max);
  return v;
  }

  this.getMidsVol = function(_min, _max){
  var min = _min || 0;
  var max = _max || 100;
  var v = map(this.getRMS(this.getMix().mids), 0, self.peak_volume, min, max);
  return v;
  }

  this.getBassVol = function(_min, _max){
  var min = _min || 0;
  var max = _max || 100;
  var v = map(this.getRMS(this.getMix().bass), 0, self.peak_volume, min, max);
  return v;
  }


  function adjustFreqData(frequencyData, ammt) {
    // get frequency data, remove obsolete
    //analyserNode.getByteFrequencyData(frequencyData);

    frequencyData.slice(0,frequencyData.length/2);
    var new_length = ammt || 16;
    var newFreqs = [], prevRangeStart = 0, prevItemCount = 0;
    // looping for my new 16 items
    for (let j=1; j<=new_length; j++) {
      // define sample size
    var pow, itemCount, rangeStart;
    if (j%2 === 1) {
      pow = (j-1)/2;
    } else {
      pow = j/2;
    }
    itemCount = Math.pow(2, pow);
    if (prevItemCount === 1) {
      rangeStart = 0;
    } else {
      rangeStart = prevRangeStart + (prevItemCount/2);
    }

        // get average value, add to new array
    var newValue = 0, total = 0;
    for (let k=rangeStart; k<rangeStart+itemCount; k++) {
      // add up items and divide by total
      total += frequencyData[k];
      newValue = total/itemCount;
    }
    newFreqs.push(newValue);
    // update
    prevItemCount = itemCount;
    prevRangeStart = rangeStart;
    }
    return newFreqs;
  }


  this.matchNote = function (freq) {
    var closest = "A#1"; // Default closest note
    var closestFreq = 58.2705;
    for (var key in notes) { // Iterates through note look-up table
        // If the current note in the table is closer to the given
        // frequency than the current "closest" note, replace the
        // "closest" note.
        if (Math.abs(notes[key] - freq) <= Math.abs(notes[closest] -
                freq)) {
            closest = key;
            closestFreq = notes[key];
        }
        // Stop searching once the current note in the table is of higher
        // frequency than the given frequency.
        if (notes[key] > freq) {
            break;
        }
    }

    return [closest, closestFreq];
  }


  return this;

};



var Mic = new Microphone();
</script>
</body>
</html>
